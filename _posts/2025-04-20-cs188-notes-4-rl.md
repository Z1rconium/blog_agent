---
layout: post
title: "CS188 笔记 4：强化学习概述"
date: 2025-04-20 00:00:00 +0800
categories: [notes]
tags: [reinforcement-learning, cs188]
---

这篇文章是我在学习 UC Berkeley 的 CS188 课程时整理的第四篇笔记，
主题是 **强化学习**（Reinforcement Learning，RL）。 通过对课程
材料的总结，我检理了 RL 的基本概念以及常用的算法类别。

## 强化学习的两大维度

课程中指出，强化学习的研究可以从两个维度割分：

* **基于模型 vs. 无模型**：基于模型的方法假定我们可以访问或
  学习环境的转移概率和奖励函数，从而在计算策略时利用这些
  信息；无模型方法则直接通过与环境交互来估计价值函数或
  策略【682901096339268†L72-L79】。
* **被动 vs. 主动**：被动学习关注在固定策略下评估状态价值，
  而主动学习需要在学习过程中改进策略，在探索与利用之间进行
  权衡【682901096339268†L72-L79】。

## 经典算法

在这两大维度的组合下，出现了多种经典算法：

* **动态规划**（DP）：依赖完整的环境模型，通过迭代求解价值
  函数以产生最优策略。
* **蒙特卡洛方法**：无需模型，基于样本轨迹估计回报；可用于
  被动或主动设置。
* **时序差分学习**（TD）：结合了 DP 的引导思想和蒙特卡洛的
  经验估计，在无模型的情况下有效地学习价值函数，例如
  Q‑Learning【682901096339268†L72-L79】。

这只是强化学习的基础入门，后续笔记会深入探讨策略梯度、Actor–Critic
等更高级的主题。
